{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-metrics\n",
      "  Downloading https://files.pythonhosted.org/packages/01/07/ec2bde74ed865b56cbeb44fc1cf414d17cc828ac68e4998a8106717d282b/keras_metrics-0.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Keras>=2.1.5 in /home/raymondleemids/anaconda3/lib/python3.6/site-packages (from keras-metrics) (2.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/raymondleemids/anaconda3/lib/python3.6/site-packages (from Keras>=2.1.5->keras-metrics) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/raymondleemids/anaconda3/lib/python3.6/site-packages (from Keras>=2.1.5->keras-metrics) (1.14.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/raymondleemids/anaconda3/lib/python3.6/site-packages (from Keras>=2.1.5->keras-metrics) (1.0.5)\n",
      "Requirement already satisfied: pyyaml in /home/raymondleemids/anaconda3/lib/python3.6/site-packages (from Keras>=2.1.5->keras-metrics) (3.12)\n",
      "Requirement already satisfied: h5py in /home/raymondleemids/anaconda3/lib/python3.6/site-packages (from Keras>=2.1.5->keras-metrics) (2.7.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/raymondleemids/anaconda3/lib/python3.6/site-packages (from Keras>=2.1.5->keras-metrics) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/raymondleemids/anaconda3/lib/python3.6/site-packages (from Keras>=2.1.5->keras-metrics) (1.0.6)\n",
      "Installing collected packages: keras-metrics\n",
      "Successfully installed keras-metrics-0.0.5\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install pydot\n",
    "# !pip install keras-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import metrics\n",
    "import pydot\n",
    "\n",
    "## Plot\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib as plt\n",
    "# import matplotlib.pyplot\n",
    "\n",
    "# # NLTK\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "# from sklearn import metrics\n",
    "# from numpy import util\n",
    "\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile # not working\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load orignal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_csv= pd.read_csv('final_shuffled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenize. \n",
    "\n",
    "# original_tokenizer = Tokenizer()\n",
    "# original_tokenizer.fit_on_texts(original_csv['8K_Content'])\n",
    "\n",
    "\n",
    "# original_seq_len = 400 # set sequence length\n",
    "# original_sequences = original_tokenizer.texts_to_sequences(original_csv['8K_Content'])\n",
    "# original_data = pad_sequences(original_sequences, \n",
    "#                      maxlen=original_seq_len, padding='post', \n",
    "#                      truncating='post') # takes about 5-10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_csv['8K_Content_sequences'] = original_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_csv.to_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_csv.to_csv('final_shuffled_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_csv['Dataset_Tag'] = original_csv[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampled_csv = pd.read_csv('final_downsampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Downsampled Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model 3,000 sequence\n",
    "## not sure if i still have model json and h5, but we do have screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LSTM and LSTM CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 400 word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_csv = downsampled_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenize words\n",
    "\n",
    "# baseline_tokenizer = Tokenizer()\n",
    "# baseline_tokenizer.fit_on_texts(baseline_csv['8K_Content'])\n",
    "\n",
    "\n",
    "# baseline_seq_len = 400 # set sequence length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_sequences = baseline_tokenizer.texts_to_sequences(baseline_csv['8K_Content'])\n",
    "# baseline_data = pad_sequences(baseline_sequences, \n",
    "#                      maxlen=baseline_seq_len, padding='post', \n",
    "#                      truncating='post') # takes about 5-10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_csv['8K_Content_sequences'] = baseline_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_csv.to_csv('baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_csv = pd.read_csv('baseline.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 02-08 train. 09-10 develoment, 11-12 test\n",
    "# # X_train = final_csv['8K_Content_cleaned'][final_csv['Year'] <= 2008]\n",
    "# # X_dev = final_csv['8K_Content_cleaned'][(final_csv['Year'] >= 2009) & \n",
    "# #                                        (final_csv['Year'] <= 2010)]\n",
    "# # X_test = final_csv['8K_Content_cleaned'][final_csv['Year'] >= 2011]\n",
    "\n",
    "# X_train = baseline_csv['8K_Content_sequences'][baseline_csv['Year'] <= 2008]\n",
    "# X_dev = baseline_csv['8K_Content_sequences'][(baseline_csv['Year'] >= 2009) & \n",
    "#                                        (baseline_csv['Year'] <= 2010)]\n",
    "# X_test = baseline_csv['8K_Content_sequences'][baseline_csv['Year'] >= 2011]\n",
    "\n",
    "# y_train = baseline_csv[['down','stay','up']][baseline_csv['Year'] <= 2008]\n",
    "# y_dev = baseline_csv[['down','stay','up']][(baseline_csv['Year'] >= 2009) & \n",
    "#                                        (baseline_csv['Year'] <= 2010)]\n",
    "# y_test = baseline_csv[['down','stay','up']][baseline_csv['Year'] >= 2011]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# # precision, f1, recall\n",
    "# # load model\n",
    "# # note: latest file updated 11/30. 3,000 word sequence.\n",
    "\n",
    "# # load json and create model\n",
    "# json_file = open('model_lstm_1.json', 'r')\n",
    "# baseline_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# baseline_model = model_from_json(baseline_model_json)\n",
    "\n",
    "# # load weights into new model\n",
    "# baseline_model.load_weights(\"model_lstm_1.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 35.14%\n"
     ]
    }
   ],
   "source": [
    "# # evaluate loaded model on test data\n",
    "# baseline_model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "#                        metrics=['accuracy'])\n",
    "\n",
    "# # loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# baseline_score = baseline_model.evaluate(np.array(X_test.values.tolist()), \n",
    "#                               np.array(y_test.values.tolist()), verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (baseline_model.metrics_names[1], baseline_score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy is low because used different tokenizer method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 100)          92619500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 92,700,203\n",
      "Trainable params: 92,700,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(baseline_model, \n",
    "#            to_file='baseline_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predictions\n",
    "\n",
    "# predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and LSTM CNN: Downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_Downsampled_IncreaseSeqLen.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_Downsampled_DecreaseSeqLen.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampled_tokenizer = Tokenizer()\n",
    "# downsampled_tokenizer.fit_on_texts(downsampled_csv['8K_Content'])\n",
    "\n",
    "\n",
    "# downsampled_seq_len = 300 # set sequence length\n",
    "# downsampled_sequences = downsampled_tokenizer.texts_to_sequences(downsampled_csv['8K_Content'])\n",
    "# downsampled_data = pad_sequences(downsampled_sequences, \n",
    "#                      maxlen=downsampled_seq_len, padding='post', \n",
    "#                      truncating='post') # takes about 5-10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampled_csv['8K_Content_sequences'] = downsampled_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_csv = pd.read_csv('downsampled_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this for downsampled\n",
    "\n",
    "X_train = downsampled_csv['8K_Content_sequences'][downsampled_csv['Year'] <= 2008]\n",
    "X_dev = downsampled_csv['8K_Content_sequences'][(downsampled_csv['Year'] >= 2009) & \n",
    "                                       (downsampled_csv['Year'] <= 2010)]\n",
    "X_test = downsampled_csv['8K_Content_sequences'][downsampled_csv['Year'] >= 2011]\n",
    "\n",
    "y_train = downsampled_csv[['down','stay','up']][downsampled_csv['Year'] <= 2008]\n",
    "y_dev = downsampled_csv[['down','stay','up']][(downsampled_csv['Year'] >= 2009) & \n",
    "                                       (downsampled_csv['Year'] <= 2010)]\n",
    "y_test = downsampled_csv[['down','stay','up']][downsampled_csv['Year'] >= 2011]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampled_csv.to_csv('downsampled_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk:  model_lstm_4.json and model_lstm_4.h5\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "model_num = 4\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model_lstm_%d.json' %model_num, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_lstm_%d.h5\" %model_num)\n",
    "print(\"Loaded model from disk: \", 'model_lstm_%d.json and model_lstm_%d.h5' %(model_num, model_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_accuracy: 44.36%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                     metrics=[metrics.categorical_accuracy])\n",
    "# loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(np.array(X_test.values.tolist()), \n",
    "                              np.array(y_test.values.tolist()), verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_Downsampled_DecreaseSeqLen_IncreaseEpochs.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_Downsampled_DecreaseSeqLen_IncreaseEpochs_IncreaseDropout.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
